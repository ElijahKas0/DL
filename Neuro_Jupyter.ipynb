{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20423d09-779a-4128-a83f-5140507ae9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2440c7d-2d4a-4902-a3ab-81f44fea3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс для полносвязного слоя нейронной сети\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size, activation=None):\n",
    "        # Инициализация весов слоя с небольшими случайными значениями (равномерное распределение)\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        # Инициализация смещений слоя, все смещения устанавливаем в 0\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "        # Присваиваем функцию активации\n",
    "        self.activation = activation\n",
    "        # Инициализация переменных для оптимизации с использованием моментов (momentum)\n",
    "        self.velocity_w = np.zeros_like(self.weights)\n",
    "        self.velocity_b = np.zeros_like(self.biases)\n",
    "\n",
    "        # Для алгоритма оптимизации Adam\n",
    "        self.m_w = np.zeros_like(self.weights)  \n",
    "        self.v_w = np.zeros_like(self.weights)  \n",
    "        self.m_b = np.zeros_like(self.biases)   \n",
    "        self.v_b = np.zeros_like(self.biases)   \n",
    "        self.t = 0  \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Прямой проход через слой нейросети\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.z = np.dot(inputs, self.weights) + self.biases  \n",
    "        self.a = self.activation.forward(self.z) if self.activation else self.z  \n",
    "        return self.a\n",
    "\n",
    "    def backward(self, grad_output, learning_rate, optimizer, momentum=0.9, clip_value=1.0, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Обратный проход для вычисления градиентов и обновления параметров\n",
    "        \"\"\"\n",
    "        # Если есть функция активации, вычисляем градиенты для неё\n",
    "        grad_activation = self.activation.backward(grad_output, self.z) if self.activation else grad_output\n",
    "        \n",
    "        # Вычисляем градиенты для весов и смещений\n",
    "        grad_weights = np.dot(self.inputs.T, grad_activation)\n",
    "        grad_biases = np.sum(grad_activation, axis=0, keepdims=True)\n",
    "        \n",
    "        # Градиенты на входе слоя (нужны для предыдущих слоёв)\n",
    "        grad_inputs = np.dot(grad_activation, self.weights.T)\n",
    "        \n",
    "        # Если используется обрезание градиентов, применяем его\n",
    "        if optimizer == 'gradient_clipping':\n",
    "            grad_weights = np.clip(grad_weights, -clip_value, clip_value)\n",
    "            grad_biases = np.clip(grad_biases, -clip_value, clip_value)\n",
    "        \n",
    "        # Обновление параметров в зависимости от выбранного оптимизатора\n",
    "        if optimizer == 'sgd':  # Stochastic Gradient Descent (Градиентный спуск)\n",
    "            self.weights -= learning_rate * grad_weights\n",
    "            self.biases -= learning_rate * grad_biases\n",
    "        elif optimizer == 'momentum':  # Метод с моментумом\n",
    "            self.velocity_w = momentum * self.velocity_w - learning_rate * grad_weights\n",
    "            self.velocity_b = momentum * self.velocity_b - learning_rate * grad_biases\n",
    "            self.weights += self.velocity_w\n",
    "            self.biases += self.velocity_b\n",
    "        elif optimizer == 'adam':  # Adam optimizer\n",
    "            self.t += 1\n",
    "            # Обновление моментов для весов и смещений\n",
    "            self.m_w = beta1 * self.m_w + (1 - beta1) * grad_weights\n",
    "            self.v_w = beta2 * self.v_w + (1 - beta2) * (grad_weights ** 2)\n",
    "            self.m_b = beta1 * self.m_b + (1 - beta1) * grad_biases\n",
    "            self.v_b = beta2 * self.v_b + (1 - beta2) * (grad_biases ** 2)\n",
    "\n",
    "            # Коррекция смещения моментов\n",
    "            m_w_hat = self.m_w / (1 - beta1 ** self.t)\n",
    "            v_w_hat = self.v_w / (1 - beta2 ** self.t)\n",
    "            m_b_hat = self.m_b / (1 - beta1 ** self.t)\n",
    "            v_b_hat = self.v_b / (1 - beta2 ** self.t)\n",
    "\n",
    "            # Обновление весов и смещений с учётом моментов\n",
    "            self.weights -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
    "            self.biases -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
    "        \n",
    "        return grad_inputs  \n",
    "\n",
    "# Класс для активации ReLU\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Прямой проход для ReLU активации.\n",
    "        \"\"\"\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad_output, x):\n",
    "        \"\"\"\n",
    "        Обратный проход для ReLU активации\n",
    "        \"\"\"\n",
    "        return grad_output * (x > 0)  \n",
    "\n",
    "# Класс для активации Sigmoid\n",
    "class Sigmoid:\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Прямой проход для Sigmoid активации\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))  \n",
    "\n",
    "    def backward(self, grad_output, x):\n",
    "        \"\"\"\n",
    "        Обратный проход для Sigmoid активации\n",
    "        \"\"\"\n",
    "        sigmoid_x = self.forward(x)  \n",
    "        return grad_output * sigmoid_x * (1 - sigmoid_x)  \n",
    "\n",
    "# Класс для активации Softmax\n",
    "class Softmax:\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Прямой проход для Softmax активации\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  \n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)  \n",
    "\n",
    "    def backward(self, grad_output, x):\n",
    "        \"\"\"\n",
    "        Обратный проход для Softmax активации\n",
    "        \"\"\"\n",
    "        return grad_output  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b510fd20-83da-4883-9c1f-9ea96fe2470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс для нейронной сети\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Инициализация нейронной сети\n",
    "        \"\"\"\n",
    "        self.layers = []  \n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        \"\"\"\n",
    "        Добавление слоя в нейронную сеть\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Прямой проход через нейронную сеть\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)  \n",
    "        return x  \n",
    "    \n",
    "    def backward(self, loss_grad, learning_rate, optimizer='sgd', momentum=0.9, clip_value=1.0):\n",
    "        \"\"\"\n",
    "        Обратный проход через нейронную сеть (обратное распространение ошибки)\n",
    "        \"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_grad = layer.backward(loss_grad, learning_rate, optimizer, momentum, clip_value)\n",
    "    \n",
    "    def train(self, X, y, loss_fn, loss_fn_deriv, epochs=100, learning_rate=0.01, optimizer='sgd', momentum=0.9, batch_size=None, clip_value=1.0):\n",
    "        \"\"\"\n",
    "        Тренировка нейронной сети\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            X, y = shuffle_data(X, y)\n",
    "            \n",
    "            if batch_size:\n",
    "                batches = create_minibatches(X, y, batch_size)\n",
    "            else:\n",
    "                batches = [(X, y)]\n",
    "            \n",
    "            # Для каждого мини-батча:\n",
    "            for X_batch, y_batch in batches:\n",
    "                output = self.forward(X_batch)\n",
    "                \n",
    "                loss = loss_fn(y_batch, output)\n",
    "                \n",
    "                loss_grad = loss_fn_deriv(y_batch, output)\n",
    "                \n",
    "                self.backward(loss_grad, learning_rate, optimizer, momentum, clip_value)\n",
    "            \n",
    "            # Каждые 10 эпох выводим информацию о текущем значении ошибки\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07c6eaf-607f-4fbe-9f6b-d2d58174a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для перемешивания данных\n",
    "def shuffle_data(X, y):\n",
    "    \"\"\"\n",
    "    Перемешивает данные X и соответствующие метки y случайным образом\n",
    "    \"\"\"\n",
    "    indices = np.arange(X.shape[0])  \n",
    "    np.random.shuffle(indices)  \n",
    "    return X[indices], y[indices]  \n",
    "\n",
    "# Функция для создания мини-батчей\n",
    "def create_minibatches(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Разбивает данные X и метки y на мини-батчи заданного размера\n",
    "    \"\"\"\n",
    "    batches = []  \n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        X_batch = X[i:i+batch_size]  \n",
    "        y_batch = y[i:i+batch_size]  \n",
    "        batches.append((X_batch, y_batch))  \n",
    "    return batches  \n",
    "\n",
    "# Функция для нормализации данных\n",
    "def normalize_data(X):\n",
    "    \"\"\"\n",
    "    Нормализует данные X по каждому признаку (по колонкам), чтобы они имели нулевое среднее и стандартное отклонение 1\n",
    "    \"\"\"\n",
    "    return (X - np.mean(X, axis=0)) / np.std(X, axis=0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3774ec6e-8041-477e-814a-978528a632a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для вычисления среднеквадратичной ошибки (MSE)\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)  \n",
    "\n",
    "# Функция для вычисления производной от MSE (градиент MSE)\n",
    "def mse_loss_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_true.size  \n",
    "\n",
    "# Функция для вычисления функции потерь кросс-энтропии\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]  \n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-9)) / m  \n",
    "\n",
    "# Функция для вычисления производной от функции потерь кросс-энтропии\n",
    "def cross_entropy_loss_derivative(y_true, y_pred):\n",
    "    return (y_pred - y_true) / y_true.shape[0]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af7a9379-36fb-47fc-9e3a-839126bb91c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка датасета Iris\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# One-hot encoding для меток классов\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Разделение на тренировочные и тестовые данные\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Нормализация данных\n",
    "X_train = normalize_data(X_train)\n",
    "X_test = normalize_data(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d01252a-00a4-4c16-b530-ebe1d62bbc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.10246263943977\n",
      "Epoch 10, Loss: 0.9828027515517984\n",
      "Epoch 20, Loss: 0.36336118037499543\n",
      "Epoch 30, Loss: 0.28593004047673337\n",
      "Epoch 40, Loss: 0.199333415409687\n",
      "Epoch 50, Loss: 0.2531552356335597\n",
      "Epoch 60, Loss: 0.10973883640342524\n",
      "Epoch 70, Loss: 0.0570426667362058\n",
      "Epoch 80, Loss: 0.03186251295449231\n",
      "Epoch 90, Loss: 0.03241482967989205\n",
      "Epoch 100, Loss: 0.12564495122381356\n",
      "Epoch 110, Loss: 0.02648417588628122\n",
      "Epoch 120, Loss: 0.0030304053244566477\n",
      "Epoch 130, Loss: 0.12140965350124083\n",
      "Epoch 140, Loss: 0.02311621302578687\n",
      "Epoch 150, Loss: 0.1973675374711001\n",
      "Epoch 160, Loss: 0.21033673413244972\n",
      "Test Accuracy(точность) на Iris датасете: 96.67%\n"
     ]
    }
   ],
   "source": [
    "# Создание и настройка нейросети\n",
    "model = NeuralNetwork()\n",
    "model.add_layer(DenseLayer(4, 10, activation=ReLU()))\n",
    "model.add_layer(DenseLayer(10, 3, activation=Softmax()))\n",
    "\n",
    "# Обучение модели\n",
    "model.train(X_train, y_train, loss_fn=cross_entropy_loss, loss_fn_deriv=cross_entropy_loss_derivative,\n",
    "            epochs=170, learning_rate=0.01, optimizer='momentum', batch_size=16)\n",
    "\n",
    "# Проверка на тестовых данных\n",
    "predictions = model.forward(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predicted_classes == true_classes)\n",
    "print(f\"Test Accuracy(точность) на Iris датасете: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12d50876-3db2-47af-ae62-6ed90a94b1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка датасета MNIST с OpenML\n",
    "mnist = datasets.fetch_openml('mnist_784', version=1)\n",
    "\n",
    "# Данные и метки\n",
    "X, y = mnist[\"data\"].values, mnist[\"target\"].astype(int)  # Преобразуем DataFrame в массив NumPy и метки в целые числа\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Преобразование данных\n",
    "X_train = X_train / 255.0  # Нормализация изображений (в диапазоне [0, 1])\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# One-hot encoding для меток классов\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = encoder.fit_transform(y_train.values.reshape(-1, 1))  # Преобразуем y_train в массив NumPy\n",
    "y_test = encoder.transform(y_test.values.reshape(-1, 1))  # Преобразуем y_test в массив NumPy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "464644b3-b397-4777-a1e0-833684377b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.33104928574330184\n",
      "Epoch 10, Loss: 0.04653202634551339\n",
      "Epoch 20, Loss: 0.007269233832305614\n",
      "Epoch 30, Loss: 0.002240629861564179\n",
      "Epoch 40, Loss: 0.0005680764779192366\n",
      "Test Accuracy(точность) на MNIST датасете: 97.88%\n"
     ]
    }
   ],
   "source": [
    "# Создание и настройка нейросети\n",
    "model = NeuralNetwork()\n",
    "model.add_layer(DenseLayer(784, 256, activation=ReLU()))\n",
    "model.add_layer(DenseLayer(256, 128, activation=ReLU()))\n",
    "model.add_layer(DenseLayer(128, 10, activation=Softmax()))\n",
    "\n",
    "# Обучение модели\n",
    "model.train(X_train, y_train, \n",
    "            loss_fn=cross_entropy_loss, \n",
    "            loss_fn_deriv=cross_entropy_loss_derivative,\n",
    "            epochs=50, \n",
    "            learning_rate=0.01, \n",
    "            optimizer='momentum', \n",
    "            momentum=0.9, \n",
    "            batch_size=64)\n",
    "\n",
    "# Проверка на тестовых данных\n",
    "predictions = model.forward(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predicted_classes == true_classes)\n",
    "print(f\"Test Accuracy(точность) на MNIST датасете: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
